# -*- coding: utf-8 -*-
"""EL4106 Proyecto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SH8NztoT8XZulfOz5Iaq-DalJhD9Nyk0

# EL4106 Proyect 1 - Style transfer with VGG16 CNN

## Preliminary configurations

### Downloading the data
"""

# Run this cell using the styletransfer22.2 account
from google.colab import drive, output
drive.mount("/content/drive")

!rm -rf /content/sample_data
!cp /content/drive/MyDrive/test2017.zip /content/
!unzip -qo /content/test2017.zip -d /content/
!rm /content/test2017.zip
!mkdir /content/img

output.clear()
!echo "Done!"

"""### Creating data structures"""

import os
import numpy as np
import torch
import torchvision
from torch import nn
from torchvision import models
from torchvision import io
import matplotlib.pyplot as plt
from PIL import Image
import time
from tqdm import tqdm


class UnsupervisedImageDataset(torch.utils.data.Dataset):
    def __init__(self, img_dir, transform=None, use_gpu=False):
        self.img_dir = img_dir
        self.image_names = os.listdir(self.img_dir)
        self.transform = transform
        self.use_gpu = use_gpu

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        image = Image.open(os.path.join(self.img_dir, self.image_names[idx])).convert("RGB")
        if self.transform is not None:
            image = self.transform(image)
        return image


class VGG16DecoderImageDataset(UnsupervisedImageDataset):
    def __init__(self, *args, encoder, **kwargs):
        super().__init__(*args, **kwargs)
        self.encoder = encoder

    def __getitem__(self, idx):
        image = Image.open(os.path.join(self.img_dir, self.image_names[idx])).convert("RGB")
        if self.transform is not None:
            image = self.transform(image)

        if self.use_gpu:
            image = image.to("cuda")
        features = self.encoder(image)
        return features, image


class VGG16DecoderImageDataloader:
    def __init__(self, dataset, batch_size, use_gpu=False):
        self.dataset = dataset
        self.batch_size = batch_size
        self.use_gpu = use_gpu

    def __len__(self):
        return int(np.ceil(len(self.dataset) / self.batch_size))

    def __iter__(self):
        self.indices = list(range(len(self.dataset)))
        np.random.default_rng().shuffle(self.indices)
        return self

    def __next__(self):
        if not self.indices:
            raise StopIteration
        indices = self.indices[:self.batch_size]
        self.indices = self.indices[self.batch_size:]
        result_feats = torch.zeros((self.batch_size, 512, 7, 7))
        result_image = torch.zeros((self.batch_size, 3, 224, 224))
        for i, idx in enumerate(indices):
            features, image = self.dataset[idx]
            result_feats[i] = features
            result_image[i] = image
        if self.use_gpu:
            result_feats = result_feats.cuda()
            result_image = result_image.cuda()
        return result_feats, result_image

"""## Model definition

### Encoder

We import the VGG16 network available in Pytorch, using the default pretrained weights. Since VGG16 contains three parts (features, average pooling and MLP), we only extract the one we care about: the features, corresponding to the convolutional layers.

Choosing the depth is also implemented, cutting off each level by the max pooling layers. This depth is specified upon instancing the `VGG16Encoder` class.
"""

class VGG16Encoder(nn.Module):
    def __init__(self, depth=5, use_gpu=False):
        super().__init__()
        all_layers = (
            models
            .vgg16(weights=models.VGG16_Weights.DEFAULT)
            .eval()
            .features
        )
        if use_gpu:
            all_layers = all_layers.to("cuda")

        # Hardcoded indices for each depth
        indices = {
            1: 4,
            2: 9,
            3: 16,
            4: 23,
            5: 30,
        }
        self.model = all_layers[:indices[depth] + 1]

    def forward(self, x):
        return self.model(x)

# This code is to see the layers in the VGG16 encoder
encoder_test = VGG16Encoder()
encoder_test.model

"""### Decoder

The decoder is created by mirroring the VGG16 convolutional layers, replacing the max pooling layers with upsampling ones.
"""

class VGG16Decoder(nn.Module):
    def __init__(self, depth=5, use_gpu=False):
        super().__init__()
        all_layers = nn.Sequential(
            nn.UpsamplingNearest2d(scale_factor=2),
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),

            nn.UpsamplingNearest2d(scale_factor=2),
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),

            nn.UpsamplingNearest2d(scale_factor=2),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),

            nn.UpsamplingNearest2d(scale_factor=2),
            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),

            nn.UpsamplingNearest2d(scale_factor=2),
            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True),
        )
        if use_gpu:
            all_layers = all_layers.to("cuda")

        # Hardcoded indices for each depth
        indices = {
            1: 6,
            2: 13,
            3: 20,
            4: 25,
            5: 30,
        }
        self.model = all_layers[:indices[depth] + 1]

    def forward(self, x):
        return self.model(x)

decoder_test = VGG16Decoder()
decoder_test.model

"""### Loss function"""

class VGG16DecoderLossFunction(nn.Module):
    def __init__(self, weight=1, show_progress=False, show_images=False, use_gpu=False):
        super().__init__()
        self.weight = weight
        self.show_progress = show_progress
        self.show_images = show_images
        self.use_gpu = use_gpu

    def forward(self, recon_image, input_image, encoder):
        if self.use_gpu:
            encoder = encoder.cuda()
            input_image = input_image.cuda()
            recon_image = recon_image.cuda()

        input_features = encoder(input_image)
        recon_features = encoder(recon_image)

        image_loss = torch.pow(torch.linalg.norm(input_image - recon_image), 2)
        feature_loss = torch.pow(torch.linalg.norm(input_features - recon_features), 2)
        total_loss = image_loss + self.weight * feature_loss

        if self.show_progress:
            if self.show_images:
                for i in range(input_image.detach().shape[0]):
                    fig, ax = plt.subplots(1, 2)
                    ax[0].imshow(input_image.detach()[i].permute(1, 2, 0).cpu().numpy())
                    ax[1].imshow(recon_image.detach()[i].permute(1, 2, 0).cpu().numpy() / torch.max(recon_image.detach().cpu()))

                    ax[0].set_title("Input image")
                    ax[1].set_title("Reconstructed image")
            print(f"""
                \rLoss = {total_loss}
                \r=========================
                \rInput                  -> {input_image.shape}
                \rInput min              -> {torch.min(input_image)}
                \rInput max              -> {torch.max(input_image)}
                \rReconstructed          -> {recon_image.shape}
                \rReconstructed min      -> {torch.min(recon_image)}
                \rReconstructed max      -> {torch.max(recon_image)}
                \rInput features         -> {input_features.shape}
                \rReconstructed features -> {recon_features.shape}
            """
            )
            plt.show()
        return total_loss

"""### Whitening and Coloring Transforms"""

#TODO: refactor variable names
def wct(alpha, cf, sf, s1f=None, beta=None):
    # content image whitening
    cf = cf.double()
    c_channels, c_width, c_height = cf.size(0), cf.size(1), cf.size(2)
    cfv = cf.view(c_channels, -1)  # c x (h x w)

    c_mean = torch.mean(cfv, 1)  # perform mean for each row
    # add dim and replicate mean on rows
    c_mean = c_mean.unsqueeze(1).expand_as(cfv)
    cfv = cfv - c_mean  # subtract mean element-wise

    c_covm = torch.mm(cfv, cfv.t()).div(
        (c_width * c_height) - 1)  # construct covariance matrix
    # singular value decomposition
    c_u, c_e, c_v = torch.svd(c_covm, some=False)

    k_c = c_channels
    for i in range(c_channels):
        if c_e[i] < 0.00001:
            k_c = i
            break
    c_d = (c_e[0:k_c]).pow(-0.5)

    w_step1 = torch.mm(c_v[:, 0:k_c], torch.diag(c_d))
    w_step2 = torch.mm(w_step1, (c_v[:, 0:k_c].t()))
    whitened = torch.mm(w_step2, cfv)

    # style image coloring
    sf = sf.double()
    _, s_width, s_heigth = sf.size(0), sf.size(1), sf.size(2)
    sfv = sf.view(c_channels, -1)

    s_mean = torch.mean(sfv, 1)
    s_mean = s_mean.unsqueeze(1).expand_as(sfv)
    sfv = sfv - s_mean

    s_covm = torch.mm(sfv, sfv.t()).div((s_width * s_heigth) - 1)
    s_u, s_e, s_v = torch.svd(s_covm, some=False)

    s_k = c_channels  # same number of channels ad content features
    for i in range(c_channels):
        if s_e[i] < 0.00001:
            s_k = i
            break
    s_d = (s_e[0:s_k]).pow(0.5)

    c_step1 = torch.mm(s_v[:, 0:s_k], torch.diag(s_d))
    c_step2 = torch.mm(c_step1, s_v[:, 0:s_k].t())
    colored = torch.mm(c_step2, whitened)

    cs0_features = colored + s_mean.resize_as_(colored)
    cs0_features = cs0_features.view_as(cf)

    # additional style coloring
    if beta:
        sf = s1f
        sf = sf.double()
        _, s_width, s_heigth = sf.size(0), sf.size(1), sf.size(2)
        sfv = sf.view(c_channels, -1)

        s_mean = torch.mean(sfv, 1)
        s_mean = s_mean.unsqueeze(1).expand_as(sfv)
        sfv = sfv - s_mean

        s_covm = torch.mm(sfv, sfv.t()).div((s_width * s_heigth) - 1)
        s_u, s_e, s_v = torch.svd(s_covm, some=False)

        s_k = c_channels
        for i in range(c_channels):
            if s_e[i] < 0.00001:
                s_k = i
                break
        s_d = (s_e[0:s_k]).pow(0.5)

        c_step1 = torch.mm(s_v[:, 0:s_k], torch.diag(s_d))
        c_step2 = torch.mm(c_step1, s_v[:, 0:s_k].t())
        colored = torch.mm(c_step2, whitened)

        cs1_features = colored + s_mean.resize_as_(colored)
        cs1_features = cs1_features.view_as(cf)

        target_features = beta * cs0_features + (1.0 - beta) * cs1_features
    else:
        target_features = cs0_features

    ccsf = alpha * target_features + (1.0 - alpha) * cf
    return ccsf.float().unsqueeze(0)


def wct_mask(cf, sf):
    cf = cf.double()
    cf_sizes = cf.size()
    c_mean = torch.mean(cf, 1)
    c_mean = c_mean.unsqueeze(1).expand_as(cf)
    cf -= c_mean

    c_covm = torch.mm(cf, cf.t()).div(cf_sizes[1] - 1)
    c_u, c_e, c_v = torch.svd(c_covm, some=False)

    k_c = cf_sizes[0]
    for i in range(cf_sizes[0]):
        if c_e[i] < 0.00001:
            k_c = i
            break
    c_d = (c_e[0:k_c]).pow(-0.5)
    whitened = torch.mm(
        torch.mm(torch.mm(c_v[:, 0:k_c], torch.diag(c_d)), (c_v[:, 0:k_c].t())), cf)

    sf = sf.double()
    sf_sizes = sf.size()
    sfv = sf.view(sf_sizes[0], sf_sizes[1] * sf_sizes[2])
    s_mean = torch.mean(sfv, 1)
    s_mean = s_mean.unsqueeze(1).expand_as(sfv)
    sfv -= s_mean

    s_covm = torch.mm(sfv, sfv.t()).div((sf_sizes[1] * sf_sizes[2]) - 1)
    s_u, s_e, s_v = torch.svd(s_covm, some=False)

    s_k = sf_sizes[0]
    for i in range(sf_sizes[0]):
        if s_e[i] < 0.00001:
            s_k = i
            break
    s_d = (s_e[0:s_k]).pow(0.5)
    ccsf = torch.mm(torch.mm(
        torch.mm(s_v[:, 0:s_k], torch.diag(s_d)), s_v[:, 0:s_k].t()), whitened)

    ccsf += s_mean.resize_as_(ccsf)
    return ccsf.float()

"""## Decoder training

### Training functions
"""

# TODO: Fix all of this code (it is copied from Tarea 2)
def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu, encoder):
    y_predicted = model(x_batch)
    if use_gpu:
        y_predicted = y_predicted.cuda()
    loss = criterion(y_predicted, y_batch, encoder)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return y_predicted, loss


def evaluate(val_loader, model, criterion, use_gpu):
    cumulative_loss = 0
    data_count = 0

    for x_val, y_val in val_loader:
        if use_gpu:
            x_val = x_val.cuda()
            y_val = y_val.cuda()

        y_predicted = model(x_val)
        loss = criterion(y_predicted, y_val, model)

        cumulative_loss += loss.item()
        data_count += y_val.shape[0]

    return cumulative_loss / len(val_loader)


def train_model(model, train_dataset, epochs, criterion,
                batch_size, lr, encoder, n_evaluations_per_epoch=6,
                use_gpu=False, loader_kwargs=None):
    if use_gpu:
        model = model.cuda()
        encoder = encoder.cuda()
        criterion = criterion.cuda()
    if loader_kwargs is None:
        loader_kwargs = {}

    train_loader = VGG16DecoderImageDataloader(
        train_dataset, batch_size=batch_size, use_gpu=use_gpu, **loader_kwargs,
    )
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    curves = {
        "train_loss": [],
    }

    initial_time = time.perf_counter()
    n_batches = len(train_loader)

    for epoch in range(epochs):
        print(f"\rEpoch {epoch + 1}/{epochs}")

        # Metrics
        cumulative_train_loss = 0
        train_loss_count = 0

        model.train()
        progress_bar = tqdm(
            enumerate(train_loader), total=n_batches, desc="Training the decoder",
        )
        for i, (x_batch, y_batch) in progress_bar:
            if use_gpu:
                x_batch = x_batch.cuda()
                y_batch = y_batch.cuda()

            y_predicted, loss = train_step(
                x_batch, y_batch, model, optimizer, criterion, use_gpu, encoder,
            )
            cumulative_train_loss += loss.item()
            train_loss_count += 1

            train_loss = cumulative_train_loss / train_loss_count
            progress_bar.desc = f"Training the decoder (train loss = {train_loss})"

        train_loss = cumulative_train_loss / train_loss_count
        curves["train_loss"].append(train_loss)

    model = model.cpu()
    return curves


def show_curves(curves):
    fig, ax = plt.subplots(1, 1, figsize=(13, 5))
    fig.set_facecolor("white")

    epochs = np.arange(len(curves["val_loss"])) + 1

    ax[0].plot(epochs, curves["val_loss"], label="validation")
    ax[0].plot(epochs, curves["train_loss"], label="training")
    ax[0].set_xlabel("Epoch")
    ax[0].set_ylabel("Loss")
    ax[0].set_title("Loss evolution during training")
    ax[0].legend()

    plt.show()

"""### Training parameters
Having defined all the code to train the network, we have to run it. We start off by creating a dataset from the images from COCO (specifically, the 2017 test dataset).
"""

LEARNING_RATE = 5e-4
BATCH_SIZE = 30
EPOCHS = 1
USE_GPU = True
NORM_MEAN = (0.485, 0.456, 0.406)
NORM_STD = (0.229, 0.224, 0.225)

transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.Normalize(NORM_MEAN, NORM_STD),
])

vgg_encoder = VGG16Encoder(use_gpu=USE_GPU)

untransformed_train_ds = VGG16DecoderImageDataset(
    "data/test2017",
    encoder=vgg_encoder,
    transform=torchvision.transforms.ToTensor(),
    use_gpu=USE_GPU,
)
train_ds = VGG16DecoderImageDataset(
    "data/test2017",
    encoder=vgg_encoder,
    transform=transform,
    use_gpu=USE_GPU,
)
reduced_ds = VGG16DecoderImageDataset(
    "data/test2017",
    encoder=vgg_encoder,
    transform=transform,
    use_gpu=USE_GPU,
)
reduced_ds.image_names = reduced_ds.image_names[:120]

# Loss function and model to train
criterion = VGG16DecoderLossFunction(1, use_gpu=USE_GPU)
vgg_decoder = VGG16Decoder(use_gpu=USE_GPU)

"""### Training execution"""

# Flush the memory in cuda before running
torch.cuda.empty_cache()

# Run the training
curves = train_model(
    vgg_decoder,
    train_ds,
    EPOCHS,
    criterion,
    BATCH_SIZE,
    LEARNING_RATE,
    vgg_encoder,
    use_gpu=USE_GPU,
)

"""## Evaluating the model

### Preprocessing
Here we compare the original with the preprocessed images, to see how it affects them.
"""

for i in range(4):
    fig, ax = plt.subplots(1, 2)
    ax[0].set_title("Imagen original")
    ax[0].imshow(untransformed_train_ds[i][1].permute(1, 2, 0).cpu())

    ax[1].set_title("Imagen transformada")
    ax[1].imshow(train_ds[i][1].permute(1, 2, 0).cpu())

    fig.savefig(f"img/transform-comparison-{i}.pdf", bbox_inches="tight")
plt.show()

INDEX = 7

feats = vgg_encoder(train_ds[INDEX][1])
img_tensor = torch.zeros((1, *feats.shape))
img_tensor[0] = feats

fig, ax = plt.subplots(1, 2)
ax[0].imshow(train_ds[INDEX][1].permute(1, 2, 0).cpu())
ax[1].imshow(vgg_decoder(img_tensor).detach()[0].permute(1, 2, 0))